{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(word_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "{'her', 'did', 'yourself', 'was', 'now', 'it', \"you'd\", 'after', 'mustn', 'too', 'ours', 'such', 'yours', 'there', 'haven', 'hadn', 'up', 'from', 'or', 'of', 'an', 'i', \"you've\", 'who', 'do', 'down', 'him', 'off', \"mustn't\", 'itself', \"weren't\", 'in', 'wouldn', 'while', 'be', 'to', 'are', 'they', 't', 'only', 'weren', 'ourselves', 'as', 'wasn', 'at', 'again', 'our', 'does', 'what', \"won't\", 'how', \"shouldn't\", \"should've\", 'we', 'with', 'didn', 'theirs', 'is', \"that'll\", 'until', 'out', 'most', 'were', 'but', 'if', 'she', 'those', 'should', 'its', \"she's\", 'hasn', 'his', \"doesn't\", 'against', 'once', 'where', 'o', 's', 'these', 'further', 'during', 'more', 'been', 'no', 'just', 'am', 'into', 'that', 'same', 'each', 'all', 'on', 'few', 'whom', 'other', 'and', 'ain', 'myself', 'about', 'under', 'the', 'here', 'herself', 'through', 'very', 'had', 'd', 'he', 'yourselves', 're', 'mightn', 'a', 'shan', 'by', 'don', \"you'll\", 'has', 'than', 'can', \"mightn't\", 'for', \"needn't\", 'shouldn', 'm', \"haven't\", \"wasn't\", 'this', \"it's\", 'y', 'ma', 'my', 'being', 'won', 'll', 'their', \"isn't\", 'have', \"don't\", 'himself', 'between', 'below', \"didn't\", \"you're\", 'both', 'not', 'couldn', 'isn', 'your', 'some', 'any', \"wouldn't\", 'because', 'when', \"hadn't\", 'so', \"aren't\", 'nor', 'you', 'doing', 'then', 'above', 'which', 'will', 'aren', 'over', 'before', \"couldn't\", 'them', 'doesn', 'needn', 'me', 'themselves', 'why', \"hasn't\", 'having', 'hers', 'own', \"shan't\", 've'}\n",
      "['work', 'play', 'makes', 'jack', 'dull', 'boy', '!', 'work', 'play', 'makes', 'jill', 'dull', 'girl', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords  # We imported auxiliary corpus\n",
    "# provided with NLTK\n",
    "data = \"All work and no play makes jack dull boy! All work and no play makes jill a dull girl.\"\n",
    "stopWords = set(stopwords.words('english'))  # a set of English stopwords\n",
    "words = word_tokenize(data.lower())\n",
    "wordsFiltered = []\n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "\n",
    "print(len(stopWords))  # Print the number of stopwords\n",
    "print(stopWords)  # Print the stopwords\n",
    "print(wordsFiltered)  # Print the filtered text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words = [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "ps = PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord = ps.stem(w)\n",
    "    print(rootWord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "student\n",
      ",\n",
      "you\n",
      "have\n",
      "to\n",
      "build\n",
      "a\n",
      "veri\n",
      "good\n",
      "applic\n",
      "and\n",
      "i\n",
      "love\n",
      "teach\n",
      "you\n",
      "natur\n",
      "languag\n",
      "process\n",
      "subject\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentence =\"Hello Students, You have to build a very good application and I love teaching you natural language processing subject.\"\n",
    "words = word_tokenize(sentence)\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for w in words:\n",
    "     rootWord = ps.stem(w)\n",
    "     print(rootWord)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming code 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for studies is studi\n",
      "Stemming for studying is studi\n",
      "Stemming for cries is cri\n",
      "Stemming for cry is cri\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization code 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "   print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['learn', 'python', 'for', 'NLP', 'programming', 'and', 'make', 'study', 'easy']\n",
      "After Token: [('learn', 'NN'), ('python', 'NN'), ('for', 'IN'), ('NLP', 'NNP'), ('programming', 'NN'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk learn/NN python/NN)\n",
      "  for/IN\n",
      "  (mychunk NLP/NNP programming/NN and/CC)\n",
      "  make/VB\n",
      "  (mychunk study/NN easy/JJ))\n",
      "After Split: ['learn', 'python', 'for', 'NLP', 'programming', 'and', 'make', 'study', 'easy']\n",
      "After Token: [('learn', 'NN'), ('python', 'NN'), ('for', 'IN'), ('NLP', 'NNP'), ('programming', 'NN'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk learn/NN python/NN)\n",
      "  for/IN\n",
      "  (mychunk NLP/NNP programming/NN and/CC)\n",
      "  make/VB\n",
      "  (mychunk study/NN easy/JJ))\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text = \"learn python for NLP programming and make study easy\".split()\n",
    "print(\"After Split:\", text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\", tokens_tag)\n",
    "patterns = \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\", chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\", output)\n",
    "\n",
    "from nltk import pos_tag\n",
    "text = \"learn python for NLP programming and make study easy\".split()\n",
    "print(\"After Split:\", text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\", tokens_tag)\n",
    "patterns = \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\", chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learn', 'python', 'from', 'Salman', 'Uddin']\n",
      "[('learn', 'JJ'), ('python', 'NN'), ('from', 'IN'), ('Salman', 'NNP'), ('Uddin', 'NNP')]\n",
      "(S (NP learn/JJ python/NN) from/IN Salman/NNP Uddin/NNP)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"learn python from Salman Uddin\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tag)\n",
    "print(result)\n",
    "result.draw()  # It will draw the pattern graphically which can be seen in Noun Phrase chunking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment-5  Word N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This', 'is', 'my', 'sentence', 'and', 'I')\n",
      "('is', 'my', 'sentence', 'and', 'I', 'want')\n",
      "('my', 'sentence', 'and', 'I', 'want', 'to')\n",
      "('sentence', 'and', 'I', 'want', 'to', 'ngramize')\n",
      "('and', 'I', 'want', 'to', 'ngramize', 'it.')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = \"This is my sentence and I want to ngramize it.\"\n",
    "n = 6\n",
    "w_6grams = ngrams(sentence.split(), n)\n",
    "for grams in w_6grams:\n",
    "    print(grams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T h i s   i\n",
      "h i s   i s\n",
      "i s   i s  \n",
      "s   i s   m\n",
      "  i s   m y\n",
      "i s   m y  \n",
      "s   m y   s\n",
      "  m y   s e\n",
      "m y   s e n\n",
      "y   s e n t\n",
      "  s e n t e\n",
      "s e n t e n\n",
      "e n t e n c\n",
      "n t e n c e\n",
      "t e n c e  \n",
      "e n c e   a\n",
      "n c e   a n\n",
      "c e   a n d\n",
      "e   a n d  \n",
      "  a n d   I\n",
      "a n d   I  \n",
      "n d   I   w\n",
      "d   I   w a\n",
      "  I   w a n\n",
      "I   w a n t\n",
      "  w a n t  \n",
      "w a n t   t\n",
      "a n t   t o\n",
      "n t   t o  \n",
      "t   t o   n\n",
      "  t o   n g\n",
      "t o   n g r\n",
      "o   n g r a\n",
      "  n g r a m\n",
      "n g r a m i\n",
      "g r a m i z\n",
      "r a m i z e\n",
      "a m i z e  \n",
      "m i z e   i\n",
      "i z e   i t\n",
      "z e   i t .\n"
     ]
    }
   ],
   "source": [
    "# Character N-gram\n",
    "from nltk import ngrams\n",
    "sentence = \"This is my sentence and I want to ngramize it.\"\n",
    "n = 6\n",
    "c_6grams = ngrams(sentence, n)\n",
    "for grams in c_6grams:\n",
    "    print(' '.join(grams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Today/NN\n",
      "  you/PRP\n",
      "  ’ll/VBP\n",
      "  be/VB\n",
      "  learning/VBG\n",
      "  (ORGANIZATION NLTK/NNP)\n",
      "  ./.)\n",
      "[Tree('ORGANIZATION', [('NLTK', 'NNP')])]\n",
      "['NLTK']\n"
     ]
    }
   ],
   "source": [
    "from nltk import chunk, tag\n",
    "sent = ['Today', 'you', \"’ll\", 'be', 'learning', 'NLTK', '.']\n",
    "tagged_sent = tag.pos_tag(sent)\n",
    "tree = chunk.ne_chunk(tagged_sent)\n",
    "print(tree)  # Tree(’S’, # *(’Today’, ’NN’), (’you’, ’PRP’),\n",
    "# (\"’ll\", ’MD’), (’be’, ’VB’), (’learning’, ’VBG’),\n",
    "# Tree(’ORGANIZATION’, *(’NLTK’, ’NNP’)+), (’.’, ’.’)+)\n",
    "ne_subtrees = tree.subtrees(filter=lambda t: t.label() in\n",
    "                            [\"ORGANIZATION\", \"PERSON\",\n",
    "                            \"LOCATION\", \"DATE\", \"TIME\",\n",
    "                             \"MONEY\", \"PERCENT\",\n",
    "                             \"FACILITY\", \"GPE\"])\n",
    "ne_subtrees_list = [tree for tree in ne_subtrees]\n",
    "print(ne_subtrees_list)\n",
    "# *Tree(’ORGANIZATION’, *(’NLTK’, ’NNP’)+)+\n",
    "ne_phrases = [' '.join(word for word, pos in tree.leaves())\n",
    "              for tree in ne_subtrees_list]\n",
    "print(ne_phrases)\n",
    "# *’NLTK’+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hi', 77), ('stats', 60), ('happy', 41), ('thanks', 37), (\"i'm\", 37), ('hello', 29), ('hey', 28), ('#ff', 26), ('good', 25), ('#followfriday', 23)]\n",
      "Accuracy is: 0.5473333333333333\n",
      "Most Informative Features\n",
      "                   happy = True           Positi : Negati =     20.4 : 1.0\n",
      "                      hi = True           Positi : Negati =     11.9 : 1.0\n",
      "                   hello = True           Positi : Negati =      9.4 : 1.0\n",
      "                  follow = True           Positi : Negati =      5.7 : 1.0\n",
      "                     hey = True           Positi : Negati =      5.0 : 1.0\n",
      "                     kik = True           Negati : Positi =      4.3 : 1.0\n",
      "                  really = True           Negati : Positi =      4.3 : 1.0\n",
      "                 there's = True           Negati : Positi =      4.3 : 1.0\n",
      "                    guys = True           Negati : Positi =      4.2 : 1.0\n",
      "                      im = True           Negati : Positi =      3.9 : 1.0\n",
      "None\n",
      "I ordered just once from TerribleCo, they screwed up, never used the app again. Negative\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words=()):\n",
    "   cleaned_tokens = []\n",
    "   for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\n",
    "                    '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\", \"\", token)\n",
    "        if tag.startswith(\"NN\"):\n",
    "          pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "          pos = 'v'\n",
    "        else:\n",
    "          pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "                cleaned_tokens.append(token.lower())\n",
    "        return cleaned_tokens\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "          yield token\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "    negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "    text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "    tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "    stop_words = stopwords.words('english')\n",
    "    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "    positive_cleaned_tokens_list = []\n",
    "    negative_cleaned_tokens_list = []\n",
    "    for tokens in positive_tweet_tokens:\n",
    "        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "    for tokens in negative_tweet_tokens:\n",
    "        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "    freq_dist_pos = FreqDist(all_pos_words)\n",
    "    print(freq_dist_pos.most_common(10))\n",
    "    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "    positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                        for tweet_dict in positive_tokens_for_model]\n",
    "    negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                        for tweet_dict in negative_tokens_for_model]\n",
    "    dataset = positive_dataset + negative_dataset\n",
    "    random.shuffle(dataset)\n",
    "    train_data = dataset[:7000]\n",
    "    test_data = dataset[7000:]\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "    print(classifier.show_most_informative_features(10))\n",
    "    custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "    \n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))    \n",
    "print(custom_tweet, classifier.classify(\n",
    "        dict([token, True] for token in custom_tokens)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82ed002fa2d4956f5c6aec99bcefe0f73a9f79882f3c9e2319b14958a5896ac5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
